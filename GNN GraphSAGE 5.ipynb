{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf721aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final model is the last one (Model_5_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb9df3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/nfs/home/UNT/mk1467/.local/lib/python3.10/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /nfs/home/UNT/mk1467/.local/lib/python3.10/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/nfs/home/UNT/mk1467/.local/lib/python3.10/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /nfs/home/UNT/mk1467/.local/lib/python3.10/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3aeb781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used to build drug_feat\n",
    "def to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6072f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used to create edge_index_supervision and edge_label_supervision\n",
    "def split_edges(edge_index, edge_label, split_ratio=0.7):\n",
    "    num_edges = edge_index.size(1)\n",
    "    random_permutation = torch.randperm(num_edges)\n",
    "    split_edge = int(split_ratio * num_edges)\n",
    "    edge_index_meg_pass = edge_index[:, random_permutation[:split_edge]]\n",
    "    edge_index_supervision = edge_index[:, random_permutation[split_edge: ]]\n",
    "    edge_label_supervision = edge_label[random_permutation[split_edge: ]]\n",
    "    return edge_index_meg_pass, edge_index_supervision, edge_label_supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ed9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainData():\n",
    "    # creating drug_feat\n",
    "    file_path = './NE Dataset/drug_attr.csv'\n",
    "    dfDrug = pd.read_csv(file_path)\n",
    "    dfDrugFeat = dfDrug.drop(dfDrug.columns[:2], axis=1)\n",
    "    dfDrugFeat.replace('#NAME?', np.nan, inplace=True) # replacing the missing values with 0\n",
    "    dfDrugFeat.fillna(0, inplace=True)\n",
    "    dfDrugFeat = dfDrugFeat.applymap(to_float)\n",
    "    dfDrugFeat.replace([np.inf], 0, inplace=True) # replacing the inf value with zero\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) # NORMALIZE HERE\n",
    "    dfDrugFeat = scaler.fit_transform(dfDrugFeat)\n",
    "    drug_feat = torch.from_numpy(dfDrugFeat).to(torch.float)\n",
    "    # creating target_feat\n",
    "    file_path = './NE Dataset/protein_attr.csv'\n",
    "    dfTarget = pd.read_csv(file_path)\n",
    "    dfTargetFeat = dfTarget.drop(dfTarget.columns[:2], axis=1)\n",
    "    target_feat = torch.from_numpy(dfTargetFeat.values).to(torch.float)\n",
    "    # unique drug and unique target mapping\n",
    "    uniqueDrug = dfDrug['1'].unique()\n",
    "    uniqueDrug = pd.DataFrame(data={\n",
    "        'DrugId': uniqueDrug,\n",
    "        'mappedId': pd.RangeIndex(len(uniqueDrug)),\n",
    "    })\n",
    "    uniqueTraget = dfTarget['#'].unique()\n",
    "    uniqueTraget = pd.DataFrame(data={\n",
    "        'TargetId': uniqueTraget,\n",
    "        'mappedId': pd.RangeIndex(len(uniqueTraget)),\n",
    "    })\n",
    "    # creating edge_index_drug_to_drug\n",
    "    file_path = './NE Dataset/train_ddi.csv'\n",
    "    dfDDI = pd.read_csv(file_path)\n",
    "    source_drug_id = pd.merge(dfDDI['Drug'], uniqueDrug,\n",
    "                             left_on='Drug', right_on='DrugId', how='left')\n",
    "    source_drug_id = torch.from_numpy(source_drug_id['mappedId'].values)\n",
    "    destination_drug_id = pd.merge(dfDDI['Target'], uniqueDrug,\n",
    "                                  left_on='Target', right_on='DrugId', how='left')\n",
    "    destination_drug_id = torch.from_numpy(destination_drug_id['mappedId'].values)\n",
    "    edge_index_drug_to_drug = torch.stack([source_drug_id, destination_drug_id], dim=0)\n",
    "    # creating edge_index_target_to_target\n",
    "    file_path = './NE Dataset/train_ppi.csv'\n",
    "    dfPPI = pd.read_csv(file_path)\n",
    "    source_target_id = pd.merge(dfPPI['Drug'], uniqueTraget,\n",
    "                               left_on='Drug', right_on='TargetId', how='left')\n",
    "    source_target_id = torch.from_numpy(source_target_id['mappedId'].values)\n",
    "    destination_target_id = pd.merge(dfPPI['Target'], uniqueTraget,\n",
    "                                    left_on='Target', right_on='TargetId', how='left')\n",
    "    destination_target_id = torch.from_numpy(destination_target_id['mappedId'].values)\n",
    "    edge_index_target_to_target = torch.stack([source_target_id, destination_target_id], dim=0)\n",
    "    # creating edge_index_pos_drug_to_target, edge_index_neg_drug_to_target, pos_edge_label, neg_edge_label\n",
    "    file_path = './NE Dataset/train_dti.csv'\n",
    "    dfDTI = pd.read_csv(file_path)\n",
    "    dfDTIpos = dfDTI[dfDTI['Label'] == 1]\n",
    "    dfDTIneg = dfDTI[dfDTI['Label'] == 0]\n",
    "    pos_interact_drug_id = pd.merge(dfDTIpos['Drug'], uniqueDrug,\n",
    "                                   left_on='Drug', right_on='DrugId', how='left')\n",
    "    pos_interact_drug_id = torch.from_numpy(pos_interact_drug_id['mappedId'].values)\n",
    "    pos_interact_target_id = pd.merge(dfDTIpos['Target'], uniqueTraget,\n",
    "                                      left_on='Target', right_on='TargetId', how='left')\n",
    "    pos_interact_target_id = torch.from_numpy(pos_interact_target_id['mappedId'].values)\n",
    "    edge_index_pos_drug_to_target = torch.stack([pos_interact_drug_id, pos_interact_target_id], dim=0) # edge_index_pos_drug_to_target\n",
    "    neg_interact_drug_id = pd.merge(dfDTIneg['Drug'], uniqueDrug,\n",
    "                                   left_on='Drug', right_on='DrugId', how='left')\n",
    "    neg_interact_drug_id = torch.from_numpy(neg_interact_drug_id['mappedId'].values)\n",
    "    neg_interact_target_id = pd.merge(dfDTIneg['Target'], uniqueTraget,\n",
    "                                     left_on='Target', right_on='TargetId', how='left')\n",
    "    neg_interact_target_id = torch.from_numpy(neg_interact_target_id['mappedId'].values)\n",
    "    edge_index_neg_drug_to_target = torch.stack([neg_interact_drug_id, neg_interact_target_id], dim=0) # edge_index_neg_drug_to_target\n",
    "    pos_edge_label = dfDTIpos['Label']\n",
    "    pos_edge_label = torch.tensor(pos_edge_label.values) # pos_edge_label\n",
    "    neg_edge_label = dfDTIneg['Label']\n",
    "    neg_edge_label = torch.tensor(neg_edge_label.values) # neg_edge_label\n",
    "    # creating edge_index_supervision and edge_label_supervision\n",
    "    edge_index_meg_pass, pos_edge_index_supervision, pos_edge_label_supervision = split_edges(edge_index_pos_drug_to_target, pos_edge_label)\n",
    "    dropped_edge_index, neg_edge_index_supervision, neg_edge_label_supervision = split_edges(edge_index_neg_drug_to_target, neg_edge_label)\n",
    "    edge_index_supervision = torch.cat((pos_edge_index_supervision, neg_edge_index_supervision), dim=1)\n",
    "    edge_label_supervision = torch.cat((pos_edge_label_supervision, neg_edge_label_supervision), dim=0)\n",
    "    random_permutation = torch.randperm(1424)\n",
    "    edge_index_supervision = edge_index_supervision[:, random_permutation]\n",
    "    edge_label_supervision = edge_label_supervision[random_permutation]\n",
    "    # creating trainData\n",
    "    # adding node and features of drug\n",
    "    trainData = HeteroData()\n",
    "    trainData['drug'].node_id = torch.arange(drug_feat.size(0))\n",
    "    trainData['drug'].x = drug_feat\n",
    "    # adding node and features of target\n",
    "    trainData['target'].node_id = torch.arange(target_feat.size(0))\n",
    "    trainData['target'].x = target_feat\n",
    "    # adding all the edges which will be used for message passing to the heterogeneous data\n",
    "    trainData[\"drug\", \"interact\", \"target\"].edge_index = edge_index_meg_pass\n",
    "    trainData[\"drug\", \"similar\", \"drug\"].edge_index = edge_index_drug_to_drug\n",
    "    trainData[\"target\", \"similar\", \"target\"].edge_index = edge_index_target_to_target\n",
    "    trainData = T.ToUndirected()(trainData) # making all the message passing edges undirected\n",
    "    # adding edge index and edge label for supervision to the data\n",
    "    trainData[\"drug\", \"interact\", \"target\"].edge_label = edge_label_supervision\n",
    "    trainData[\"drug\", \"interact\", \"target\"].edge_label_index = edge_index_supervision\n",
    "    return trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a110f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2314173/1833806588.py:4: DtypeWarning: Columns (663,786,787,832,835,869,905,906,951,954,988,1146,1411) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfDrug = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "train_data = create_trainData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce559047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mdrug\u001b[0m={\n",
       "    node_id=[1482],\n",
       "    x=[1482, 1444]\n",
       "  },\n",
       "  \u001b[1mtarget\u001b[0m={\n",
       "    node_id=[1408],\n",
       "    x=[1408, 4159]\n",
       "  },\n",
       "  \u001b[1m(drug, interact, target)\u001b[0m={\n",
       "    edge_index=[2, 1659],\n",
       "    edge_label=[1424],\n",
       "    edge_label_index=[2, 1424]\n",
       "  },\n",
       "  \u001b[1m(drug, similar, drug)\u001b[0m={ edge_index=[2, 1246640] },\n",
       "  \u001b[1m(target, similar, target)\u001b[0m={ edge_index=[2, 1043321] },\n",
       "  \u001b[1m(target, rev_interact, drug)\u001b[0m={ edge_index=[2, 1659] }\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a0d7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100, 1424):\n",
    "#     print(train_data['drug', 'interact', 'target'].edge_label[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0abe723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valData():\n",
    "    # creating drug_feat\n",
    "    file_path = './NE Dataset/drug_attr.csv'\n",
    "    dfDrug = pd.read_csv(file_path)\n",
    "    dfDrugFeat = dfDrug.drop(dfDrug.columns[:2], axis=1)\n",
    "    dfDrugFeat.replace('#NAME?', np.nan, inplace=True) # replacing the missing values with 0\n",
    "    dfDrugFeat.fillna(0, inplace=True)\n",
    "    dfDrugFeat = dfDrugFeat.applymap(to_float)\n",
    "    dfDrugFeat.replace([np.inf], 0, inplace=True) # replacing the inf value with zero\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) # NORMALIZE HERE\n",
    "    dfDrugFeat = scaler.fit_transform(dfDrugFeat)\n",
    "    drug_feat = torch.from_numpy(dfDrugFeat).to(torch.float)\n",
    "    # creating target_feat\n",
    "    file_path = './NE Dataset/protein_attr.csv'\n",
    "    dfTarget = pd.read_csv(file_path)\n",
    "    dfTargetFeat = dfTarget.drop(dfTarget.columns[:2], axis=1)\n",
    "    target_feat = torch.from_numpy(dfTargetFeat.values).to(torch.float)\n",
    "    # unique drug and unique target mapping\n",
    "    uniqueDrug = dfDrug['1'].unique()\n",
    "    uniqueDrug = pd.DataFrame(data={\n",
    "        'DrugId': uniqueDrug,\n",
    "        'mappedId': pd.RangeIndex(len(uniqueDrug)),\n",
    "    })\n",
    "    uniqueTraget = dfTarget['#'].unique()\n",
    "    uniqueTraget = pd.DataFrame(data={\n",
    "        'TargetId': uniqueTraget,\n",
    "        'mappedId': pd.RangeIndex(len(uniqueTraget)),\n",
    "    })\n",
    "    # creating edge_index_drug_to_drug\n",
    "    file_path = './NE Dataset/train_ddi.csv'\n",
    "    dfDDI = pd.read_csv(file_path)\n",
    "    source_drug_id = pd.merge(dfDDI['Drug'], uniqueDrug,\n",
    "                             left_on='Drug', right_on='DrugId', how='left')\n",
    "    source_drug_id = torch.from_numpy(source_drug_id['mappedId'].values)\n",
    "    destination_drug_id = pd.merge(dfDDI['Target'], uniqueDrug,\n",
    "                                  left_on='Target', right_on='DrugId', how='left')\n",
    "    destination_drug_id = torch.from_numpy(destination_drug_id['mappedId'].values)\n",
    "    edge_index_drug_to_drug = torch.stack([source_drug_id, destination_drug_id], dim=0)\n",
    "    # creating edge_index_target_to_target\n",
    "    file_path = './NE Dataset/train_ppi.csv'\n",
    "    dfPPI = pd.read_csv(file_path)\n",
    "    source_target_id = pd.merge(dfPPI['Drug'], uniqueTraget,\n",
    "                               left_on='Drug', right_on='TargetId', how='left')\n",
    "    source_target_id = torch.from_numpy(source_target_id['mappedId'].values)\n",
    "    destination_target_id = pd.merge(dfPPI['Target'], uniqueTraget,\n",
    "                                    left_on='Target', right_on='TargetId', how='left')\n",
    "    destination_target_id = torch.from_numpy(destination_target_id['mappedId'].values)\n",
    "    edge_index_target_to_target = torch.stack([source_target_id, destination_target_id], dim=0)\n",
    "    # creating edge_index_pos_drug_to_target [these edges will be used for message passing]\n",
    "    file_path = './NE Dataset/train_dti.csv'\n",
    "    dfDTI = pd.read_csv(file_path)\n",
    "    dfDTIpos = dfDTI[dfDTI['Label'] == 1]\n",
    "    pos_interact_drug_id = pd.merge(dfDTIpos['Drug'], uniqueDrug,\n",
    "                                   left_on='Drug', right_on='DrugId', how='left')\n",
    "    pos_interact_drug_id = torch.from_numpy(pos_interact_drug_id['mappedId'].values)\n",
    "    pos_interact_target_id = pd.merge(dfDTIpos['Target'], uniqueTraget,\n",
    "                                      left_on='Target', right_on='TargetId', how='left')\n",
    "    pos_interact_target_id = torch.from_numpy(pos_interact_target_id['mappedId'].values)\n",
    "    edge_index_pos_drug_to_target = torch.stack([pos_interact_drug_id, pos_interact_target_id], dim=0)\n",
    "    # creating supervision edge and label\n",
    "    file_path = './NE Dataset/val_dti.csv'\n",
    "    dfvalDTI = pd.read_csv(file_path)\n",
    "    edge_index_supervision_drug = pd.merge(dfvalDTI['Drug'], uniqueDrug,\n",
    "                                          left_on='Drug', right_on='DrugId', how='left')\n",
    "    edge_index_supervision_drug = torch.from_numpy(edge_index_supervision_drug['mappedId'].values)\n",
    "    edge_index_supervision_target = pd.merge(dfvalDTI['Target'], uniqueTraget,\n",
    "                                            left_on='Target', right_on='TargetId', how='left')\n",
    "    edge_index_supervision_target = torch.from_numpy(edge_index_supervision_target['mappedId'].values)\n",
    "    edge_index_supervision = torch.stack([edge_index_supervision_drug, edge_index_supervision_target], dim=0)\n",
    "    edge_label_supervision = dfvalDTI['Label']\n",
    "    edge_label_supervision = torch.tensor(edge_label_supervision.values)\n",
    "    random_permutationVal = torch.randperm(948)\n",
    "    edge_index_supervision = edge_index_supervision[:, random_permutationVal]\n",
    "    edge_label_supervision = edge_label_supervision[random_permutationVal]\n",
    "    # creating the heterogeneous graph valData\n",
    "    valData = HeteroData()\n",
    "    # adding node and features of drug\n",
    "    valData['drug'].node_id = torch.arange(drug_feat.size(0))\n",
    "    valData['drug'].x = drug_feat\n",
    "    # adding node and features of target\n",
    "    valData['target'].node_id = torch.arange(target_feat.size(0))\n",
    "    valData['target'].x = target_feat\n",
    "    # adding all the edges which will be used for message passing to the heterogeneous data\n",
    "    valData[\"drug\", \"interact\", \"target\"].edge_index = edge_index_pos_drug_to_target\n",
    "    valData[\"drug\", \"similar\", \"drug\"].edge_index = edge_index_drug_to_drug\n",
    "    valData[\"target\", \"similar\", \"target\"].edge_index = edge_index_target_to_target\n",
    "    # making all the edges undirected\n",
    "    valData = T.ToUndirected()(valData)\n",
    "    # adding edge index and edge label for supervision to the data\n",
    "    valData[\"drug\", \"interact\", \"target\"].edge_label = edge_label_supervision\n",
    "    valData[\"drug\", \"interact\", \"target\"].edge_label_index = edge_index_supervision\n",
    "    return valData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42f2388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2314173/1106223255.py:4: DtypeWarning: Columns (663,786,787,832,835,869,905,906,951,954,988,1146,1411) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfDrug = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "val_data = create_valData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad0f74d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mdrug\u001b[0m={\n",
       "    node_id=[1482],\n",
       "    x=[1482, 1444]\n",
       "  },\n",
       "  \u001b[1mtarget\u001b[0m={\n",
       "    node_id=[1408],\n",
       "    x=[1408, 4159]\n",
       "  },\n",
       "  \u001b[1m(drug, interact, target)\u001b[0m={\n",
       "    edge_index=[2, 2371],\n",
       "    edge_label=[948],\n",
       "    edge_label_index=[2, 948]\n",
       "  },\n",
       "  \u001b[1m(drug, similar, drug)\u001b[0m={ edge_index=[2, 1246640] },\n",
       "  \u001b[1m(target, similar, target)\u001b[0m={ edge_index=[2, 1043321] },\n",
       "  \u001b[1m(target, rev_interact, drug)\u001b[0m={ edge_index=[2, 2371] }\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c98ca1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ba0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f8271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# # Define seed edges:\n",
    "# edge_label_index = train_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "# edge_label = train_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "\n",
    "# train_loader = LinkNeighborLoader(\n",
    "#     data=train_data,\n",
    "#     num_neighbors=[20, 10],\n",
    "#     neg_sampling_ratio=0.0,\n",
    "#     edge_label_index=((\"drug\", \"interact\", \"target\"), edge_label_index),\n",
    "#     edge_label=edge_label,\n",
    "#     batch_size=8,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13bca4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect a sample:\n",
    "# sampled_data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8f28a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_3(torch.nn.Module):\n",
    "    def __init__(self, final_dimension):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(final_dimension, final_dimension)\n",
    "        self.conv2 = SAGEConv(final_dimension, final_dimension)\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x_drug: Tensor, x_target: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_drug = x_drug[edge_label_index[0]]\n",
    "        edge_feat_target = x_target[edge_label_index[1]]\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (edge_feat_drug * edge_feat_target).sum(dim=-1)\n",
    "    \n",
    "class Model_3_dot(torch.nn.Module):\n",
    "    def __init__(self, final_dimension):\n",
    "        super().__init__()\n",
    "        # embeddings ???\n",
    "        # self.user_lin = torch.nn.Linear(final_dimension, final_dimension)\n",
    "        # self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, final_dimension)\n",
    "        # self.movie_emb = torch.nn.Embedding(data[\"movie\"].num_nodes, final_dimension)\n",
    "        \n",
    "        self.drug_lin = torch.nn.Linear(1444, final_dimension)\n",
    "        self.target_lin = torch.nn.Linear(4159, final_dimension)\n",
    "        # Instantiate homogeneous GNN\n",
    "        self.gnn = GNN_3(final_dimension)\n",
    "        # Convert GNN model into a heterogeneous variant\n",
    "        self.gnn = to_hetero(self.gnn, metadata=train_data.metadata())\n",
    "        self.classifier = Classifier()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, data: HeteroData) -> Tensor:\n",
    "#         x_dict = {\n",
    "#             \"user\": self.shared_lin(self.user_emb(data[\"user\"].node_id)),\n",
    "#             \"movie\": self.shared_lin(self.movie_lin(data[\"movie\"].x) + self.movie_emb(data[\"movie\"].node_id)),\n",
    "        \n",
    "#         }\n",
    "\n",
    "        x_dict = {\n",
    "            \"drug\": self.drug_lin(data[\"drug\"].x),\n",
    "            \"target\": self.target_lin(data[\"target\"].x),\n",
    "        \n",
    "        }\n",
    "        x_dict = self.gnn(x_dict, train_data.edge_index_dict)\n",
    "        return x_dict\n",
    "\n",
    "    def predict_link(self, drug_emb, target_emb, edge_label_index):\n",
    "        return self.classifier(drug_emb, target_emb, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6bb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_dot = Model_3_dot(final_dimension=5000)\n",
    "model3_dot = model3_dot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47e277ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer_dot = Adam(model3_dot.parameters(), lr=0.001)\n",
    "loss_fn_dot = torch.nn.BCEWithLogitsLoss() # ????BCEwithLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f76dbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2291b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dot():\n",
    "    model3_dot.train()\n",
    "    for epoch in range(1, 200):\n",
    "        optimizer_dot.zero_grad()\n",
    "        x_dict = model3_dot(train_data)\n",
    "        edge_label_index = train_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "        ground_truth = train_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "        ground_truth = ground_truth.float()\n",
    "        pred = model3_dot.predict_link( x_dict['drug'], x_dict['target'], edge_label_index).squeeze()\n",
    "        # print(type(pred[0].item()))\n",
    "        # print(type(ground_truth[0].item()))\n",
    "        loss = loss_fn_dot(pred, ground_truth)\n",
    "        loss.backward()\n",
    "        optimizer_dot.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a93bfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 492.58856201171875\n",
      "Epoch: 002, Loss: 30413854.0\n",
      "Epoch: 003, Loss: 119852952.0\n",
      "Epoch: 004, Loss: 2533610.75\n",
      "Epoch: 005, Loss: 3540533.5\n",
      "Epoch: 006, Loss: 3676059.25\n",
      "Epoch: 007, Loss: 127357.3359375\n",
      "Epoch: 008, Loss: 1592586.75\n",
      "Epoch: 009, Loss: 2450857.75\n",
      "Epoch: 010, Loss: 883291.5\n",
      "Epoch: 011, Loss: 648540.5625\n",
      "Epoch: 012, Loss: 1782074.25\n",
      "Epoch: 013, Loss: 134228.28125\n",
      "Epoch: 014, Loss: 139038.1875\n",
      "Epoch: 015, Loss: 6377420.5\n",
      "Epoch: 016, Loss: 1505187.125\n",
      "Epoch: 017, Loss: 281080.3125\n",
      "Epoch: 018, Loss: 118502.0703125\n",
      "Epoch: 019, Loss: 218558.234375\n",
      "Epoch: 020, Loss: 132943.390625\n",
      "Epoch: 021, Loss: 146154.421875\n",
      "Epoch: 022, Loss: 103381.6875\n",
      "Epoch: 023, Loss: 44247.6484375\n",
      "Epoch: 024, Loss: 50762.03515625\n",
      "Epoch: 025, Loss: 25281.853515625\n",
      "Epoch: 026, Loss: 53893.5234375\n",
      "Epoch: 027, Loss: 23381.67578125\n",
      "Epoch: 028, Loss: 12042.5478515625\n",
      "Epoch: 029, Loss: 13747.646484375\n",
      "Epoch: 030, Loss: 5961.2783203125\n",
      "Epoch: 031, Loss: 2180.607177734375\n",
      "Epoch: 032, Loss: 3198.609619140625\n",
      "Epoch: 033, Loss: 2554.69140625\n",
      "Epoch: 034, Loss: 1179.5701904296875\n",
      "Epoch: 035, Loss: 553.7466430664062\n",
      "Epoch: 036, Loss: 100.44784545898438\n",
      "Epoch: 037, Loss: 10.638510704040527\n",
      "Epoch: 038, Loss: 11.08632755279541\n",
      "Epoch: 039, Loss: 10.124536514282227\n",
      "Epoch: 040, Loss: 9.451541900634766\n",
      "Epoch: 041, Loss: 8.82026195526123\n",
      "Epoch: 042, Loss: 8.08934497833252\n",
      "Epoch: 043, Loss: 7.258646488189697\n",
      "Epoch: 044, Loss: 6.361750602722168\n",
      "Epoch: 045, Loss: 5.451232433319092\n",
      "Epoch: 046, Loss: 4.501830577850342\n",
      "Epoch: 047, Loss: 3.5086214542388916\n",
      "Epoch: 048, Loss: 2.474323034286499\n",
      "Epoch: 049, Loss: 1.41462242603302\n",
      "Epoch: 050, Loss: 1.4626408815383911\n",
      "Epoch: 051, Loss: 1.8910491466522217\n",
      "Epoch: 052, Loss: 2.2348036766052246\n",
      "Epoch: 053, Loss: 2.510216236114502\n",
      "Epoch: 054, Loss: 2.7218220233917236\n",
      "Epoch: 055, Loss: 2.8739984035491943\n",
      "Epoch: 056, Loss: 2.9728970527648926\n",
      "Epoch: 057, Loss: 3.025383710861206\n",
      "Epoch: 058, Loss: 3.037299394607544\n",
      "Epoch: 059, Loss: 3.013493299484253\n",
      "Epoch: 060, Loss: 2.958409070968628\n",
      "Epoch: 061, Loss: 2.875577688217163\n",
      "Epoch: 062, Loss: 2.767683744430542\n",
      "Epoch: 063, Loss: 2.6365299224853516\n",
      "Epoch: 064, Loss: 2.4833223819732666\n",
      "Epoch: 065, Loss: 2.308924913406372\n",
      "Epoch: 066, Loss: 2.114231586456299\n",
      "Epoch: 067, Loss: 1.9003225564956665\n",
      "Epoch: 068, Loss: 1.6688235998153687\n",
      "Epoch: 069, Loss: 1.4214024543762207\n",
      "Epoch: 070, Loss: 1.1598482131958008\n",
      "Epoch: 071, Loss: 1.1021003723144531\n",
      "Epoch: 072, Loss: 1.4402153491973877\n",
      "Epoch: 073, Loss: 1.5944502353668213\n",
      "Epoch: 074, Loss: 1.5815852880477905\n",
      "Epoch: 075, Loss: 1.416878581047058\n",
      "Epoch: 076, Loss: 1.1143068075180054\n",
      "Epoch: 077, Loss: 1.0844316482543945\n",
      "Epoch: 078, Loss: 1.238506555557251\n",
      "Epoch: 079, Loss: 1.3390341997146606\n",
      "Epoch: 080, Loss: 1.3909761905670166\n",
      "Epoch: 081, Loss: 1.3988399505615234\n",
      "Epoch: 082, Loss: 1.3666616678237915\n",
      "Epoch: 083, Loss: 1.2981157302856445\n",
      "Epoch: 084, Loss: 1.1965045928955078\n",
      "Epoch: 085, Loss: 1.0647664070129395\n",
      "Epoch: 086, Loss: 1.0281175374984741\n",
      "Epoch: 087, Loss: 1.1528339385986328\n",
      "Epoch: 088, Loss: 1.1025876998901367\n",
      "Epoch: 089, Loss: 0.970101535320282\n",
      "Epoch: 090, Loss: 1.0230168104171753\n",
      "Epoch: 091, Loss: 1.029913067817688\n",
      "Epoch: 092, Loss: 0.994939386844635\n",
      "Epoch: 093, Loss: 0.9867255687713623\n",
      "Epoch: 094, Loss: 0.9505605101585388\n",
      "Epoch: 095, Loss: 1.0377998352050781\n",
      "Epoch: 096, Loss: 1.0848479270935059\n",
      "Epoch: 097, Loss: 1.0851279497146606\n",
      "Epoch: 098, Loss: 1.0429956912994385\n",
      "Epoch: 099, Loss: 0.9623075723648071\n",
      "Epoch: 100, Loss: 1.1214978694915771\n",
      "Epoch: 101, Loss: 1.1560550928115845\n",
      "Epoch: 102, Loss: 1.0127136707305908\n",
      "Epoch: 103, Loss: 1.0487735271453857\n",
      "Epoch: 104, Loss: 1.1410025358200073\n",
      "Epoch: 105, Loss: 1.1806013584136963\n",
      "Epoch: 106, Loss: 1.172646403312683\n",
      "Epoch: 107, Loss: 1.121317982673645\n",
      "Epoch: 108, Loss: 1.0307174921035767\n",
      "Epoch: 109, Loss: 0.9852628707885742\n",
      "Epoch: 110, Loss: 1.0330138206481934\n",
      "Epoch: 111, Loss: 0.9481492638587952\n",
      "Epoch: 112, Loss: 0.9650695323944092\n",
      "Epoch: 113, Loss: 0.9338146448135376\n",
      "Epoch: 114, Loss: 1.0647186040878296\n",
      "Epoch: 115, Loss: 1.0112358331680298\n",
      "Epoch: 116, Loss: 1.0022293329238892\n",
      "Epoch: 117, Loss: 1.0606858730316162\n",
      "Epoch: 118, Loss: 1.0672341585159302\n",
      "Epoch: 119, Loss: 1.0266945362091064\n",
      "Epoch: 120, Loss: 0.943271279335022\n",
      "Epoch: 121, Loss: 1.1360100507736206\n",
      "Epoch: 122, Loss: 1.1665959358215332\n",
      "Epoch: 123, Loss: 1.0033015012741089\n",
      "Epoch: 124, Loss: 1.052653431892395\n",
      "Epoch: 125, Loss: 1.1564576625823975\n",
      "Epoch: 126, Loss: 1.2025038003921509\n",
      "Epoch: 127, Loss: 1.1962370872497559\n",
      "Epoch: 128, Loss: 1.1426167488098145\n",
      "Epoch: 129, Loss: 1.0459505319595337\n",
      "Epoch: 130, Loss: 0.9403600692749023\n",
      "Epoch: 131, Loss: 0.9891892671585083\n",
      "Epoch: 132, Loss: 0.9605138897895813\n",
      "Epoch: 133, Loss: 0.9796040654182434\n",
      "Epoch: 134, Loss: 0.9476208090782166\n",
      "Epoch: 135, Loss: 1.014545202255249\n",
      "Epoch: 136, Loss: 0.9550995826721191\n",
      "Epoch: 137, Loss: 1.0237271785736084\n",
      "Epoch: 138, Loss: 1.0873228311538696\n",
      "Epoch: 139, Loss: 1.0947463512420654\n",
      "Epoch: 140, Loss: 1.051276445388794\n",
      "Epoch: 141, Loss: 0.9616236686706543\n",
      "Epoch: 142, Loss: 1.0813961029052734\n",
      "Epoch: 143, Loss: 1.113724946975708\n",
      "Epoch: 144, Loss: 0.937272310256958\n",
      "Epoch: 145, Loss: 1.081048846244812\n",
      "Epoch: 146, Loss: 1.1931289434432983\n",
      "Epoch: 147, Loss: 1.2430274486541748\n",
      "Epoch: 148, Loss: 1.2366528511047363\n",
      "Epoch: 149, Loss: 1.1793345212936401\n",
      "Epoch: 150, Loss: 1.0757839679718018\n",
      "Epoch: 151, Loss: 0.930193305015564\n",
      "Epoch: 152, Loss: 1.2324092388153076\n",
      "Epoch: 153, Loss: 1.3525879383087158\n",
      "Epoch: 154, Loss: 1.248563289642334\n",
      "Epoch: 155, Loss: 0.941903293132782\n",
      "Epoch: 156, Loss: 1.131096363067627\n",
      "Epoch: 157, Loss: 1.2970622777938843\n",
      "Epoch: 158, Loss: 1.3938654661178589\n",
      "Epoch: 159, Loss: 1.428219199180603\n",
      "Epoch: 160, Loss: 1.4061362743377686\n",
      "Epoch: 161, Loss: 1.3329424858093262\n",
      "Epoch: 162, Loss: 1.2134013175964355\n",
      "Epoch: 163, Loss: 1.0517452955245972\n",
      "Epoch: 164, Loss: 0.9993690848350525\n",
      "Epoch: 165, Loss: 1.1419986486434937\n",
      "Epoch: 166, Loss: 1.051559329032898\n",
      "Epoch: 167, Loss: 0.9728821516036987\n",
      "Epoch: 168, Loss: 1.0526894330978394\n",
      "Epoch: 169, Loss: 1.0700889825820923\n",
      "Epoch: 170, Loss: 1.0311063528060913\n",
      "Epoch: 171, Loss: 0.9410170912742615\n",
      "Epoch: 172, Loss: 1.0796340703964233\n",
      "Epoch: 173, Loss: 1.1023801565170288\n",
      "Epoch: 174, Loss: 0.8994907140731812\n",
      "Epoch: 175, Loss: 1.0880323648452759\n",
      "Epoch: 176, Loss: 1.2088350057601929\n",
      "Epoch: 177, Loss: 1.262210488319397\n",
      "Epoch: 178, Loss: 1.2546974420547485\n",
      "Epoch: 179, Loss: 1.1920981407165527\n",
      "Epoch: 180, Loss: 1.079586386680603\n",
      "Epoch: 181, Loss: 0.921745240688324\n",
      "Epoch: 182, Loss: 1.2256160974502563\n",
      "Epoch: 183, Loss: 1.3561115264892578\n",
      "Epoch: 184, Loss: 1.2444096803665161\n",
      "Epoch: 185, Loss: 0.9140971302986145\n",
      "Epoch: 186, Loss: 1.1363567113876343\n",
      "Epoch: 187, Loss: 1.3149160146713257\n",
      "Epoch: 188, Loss: 1.418922781944275\n",
      "Epoch: 189, Loss: 1.4557560682296753\n",
      "Epoch: 190, Loss: 1.4319156408309937\n",
      "Epoch: 191, Loss: 1.3532239198684692\n",
      "Epoch: 192, Loss: 1.2248291969299316\n",
      "Epoch: 193, Loss: 1.0513231754302979\n",
      "Epoch: 194, Loss: 0.9735457897186279\n",
      "Epoch: 195, Loss: 1.1265132427215576\n",
      "Epoch: 196, Loss: 1.0296218395233154\n",
      "Epoch: 197, Loss: 0.9664164185523987\n",
      "Epoch: 198, Loss: 1.0518156290054321\n",
      "Epoch: 199, Loss: 1.0704469680786133\n"
     ]
    }
   ],
   "source": [
    "train_dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad554b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = model3_dot(train_data.to(device))\n",
    "edge_label_index = train_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "pred = model3_dot.predict_link( x_dict['drug'], x_dict['target'], edge_label_index).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3458518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = torch.sigmoid(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8694fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_prob.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1003d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_binary = pred_prob >= 0.5\n",
    "pred_prob_binary = pred_prob_binary.float()\n",
    "pred_prob_binary_np = pred_prob_binary.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "199384e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = train_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "ground_truth = ground_truth.float()\n",
    "ground_truth_np = ground_truth.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e736b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.86      0.67       712\n",
      "         1.0       0.68      0.29      0.41       712\n",
      "\n",
      "    accuracy                           0.58      1424\n",
      "   macro avg       0.61      0.58      0.54      1424\n",
      "weighted avg       0.61      0.58      0.54      1424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(ground_truth_np, pred_prob_binary_np)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19d2518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf4f2301",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict_val = model3_dot(val_data)\n",
    "edge_label_index_val = val_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "pred_val = model3_dot.predict_link(x_dict_val['drug'], x_dict_val['target'], edge_label_index_val).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e9ca1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_prob = torch.sigmoid(pred_val)\n",
    "pred_prob_val_binary = pred_val_prob >= 0.5\n",
    "pred_prob_val_binary = pred_prob_val_binary.float()\n",
    "pred_prob_val_binary_np = pred_prob_val_binary.cpu().numpy()\n",
    "ground_truth_val = val_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "ground_truth_val_np = ground_truth_val.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57463048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.66      0.56       474\n",
      "           1       0.48      0.31      0.37       474\n",
      "\n",
      "    accuracy                           0.49       948\n",
      "   macro avg       0.48      0.49      0.47       948\n",
      "weighted avg       0.48      0.49      0.47       948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(ground_truth_val_np, pred_prob_val_binary_np)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aef2b32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob_val_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b495e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will create another model with shared lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00790299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_4(torch.nn.Module):\n",
    "    def __init__(self, final_dimension):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(final_dimension, final_dimension)\n",
    "        self.conv2 = SAGEConv(final_dimension, final_dimension)\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x_drug: Tensor, x_target: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_drug = x_drug[edge_label_index[0]]\n",
    "        edge_feat_target = x_target[edge_label_index[1]]\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (edge_feat_drug * edge_feat_target).sum(dim=-1)\n",
    "    \n",
    "class Model_4_dot(torch.nn.Module):\n",
    "    def __init__(self, final_dimension):\n",
    "        super().__init__()\n",
    "        # embeddings ???\n",
    "        # self.user_lin = torch.nn.Linear(final_dimension, final_dimension)\n",
    "        # self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, final_dimension)\n",
    "        # self.movie_emb = torch.nn.Embedding(data[\"movie\"].num_nodes, final_dimension)\n",
    "        \n",
    "        self.drug_lin = torch.nn.Linear(1444, final_dimension)\n",
    "        self.target_lin = torch.nn.Linear(4159, final_dimension)\n",
    "        self.shared_lin = torch.nn.Linear(final_dimension, final_dimension)\n",
    "        # Instantiate homogeneous GNN\n",
    "        self.gnn = GNN_4(final_dimension)\n",
    "        # Convert GNN model into a heterogeneous variant\n",
    "        self.gnn = to_hetero(self.gnn, metadata=train_data.metadata())\n",
    "        self.classifier = Classifier()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, data: HeteroData) -> Tensor:\n",
    "        x_dict = {\n",
    "            \"drug\": self.shared_lin(self.drug_lin(data[\"drug\"].x)),\n",
    "            \"target\": self.shared_lin(self.target_lin(data[\"target\"].x)),\n",
    "        \n",
    "        }\n",
    "\n",
    "        x_dict = self.gnn(x_dict, train_data.edge_index_dict)\n",
    "        return x_dict\n",
    "\n",
    "    def predict_link(self, drug_emb, target_emb, edge_label_index):\n",
    "        return self.classifier(drug_emb, target_emb, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d5bd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a08ab41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0855e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1f15d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_dot = Model_4_dot(final_dimension=5000)\n",
    "model4_dot = model4_dot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ba0a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer_dot = Adam(model4_dot.parameters(), lr=0.001)\n",
    "loss_fn_dot = torch.nn.BCEWithLogitsLoss() # ????BCEwithLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee80c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dot():\n",
    "    model4_dot.train()\n",
    "    for epoch in range(1, 300):\n",
    "        optimizer_dot.zero_grad()\n",
    "        x_dict = model4_dot(train_data)\n",
    "        edge_label_index = train_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "        ground_truth = train_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "        ground_truth = ground_truth.float()\n",
    "        pred = model4_dot.predict_link( x_dict['drug'], x_dict['target'], edge_label_index).squeeze()\n",
    "        # print(type(pred[0].item()))\n",
    "        # print(type(ground_truth[0].item()))\n",
    "        loss = loss_fn_dot(pred, ground_truth)\n",
    "        loss.backward()\n",
    "        optimizer_dot.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8863fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 76.97655487060547\n",
      "Epoch: 002, Loss: 14295515.0\n",
      "Epoch: 003, Loss: 4810861056.0\n",
      "Epoch: 004, Loss: 905814208.0\n",
      "Epoch: 005, Loss: 51440992.0\n",
      "Epoch: 006, Loss: 1693360768.0\n",
      "Epoch: 007, Loss: 408601856.0\n",
      "Epoch: 008, Loss: 299470592.0\n",
      "Epoch: 009, Loss: 139751584.0\n",
      "Epoch: 010, Loss: 650130496.0\n",
      "Epoch: 011, Loss: 1428370048.0\n",
      "Epoch: 012, Loss: 56385232896.0\n",
      "Epoch: 013, Loss: 4971673088.0\n",
      "Epoch: 014, Loss: 30785531904.0\n",
      "Epoch: 015, Loss: 22881927168.0\n",
      "Epoch: 016, Loss: 678736384.0\n",
      "Epoch: 017, Loss: 758128896.0\n",
      "Epoch: 018, Loss: 1078195840.0\n",
      "Epoch: 019, Loss: 668963072.0\n",
      "Epoch: 020, Loss: 406938432.0\n",
      "Epoch: 021, Loss: 621747392.0\n",
      "Epoch: 022, Loss: 309612512.0\n",
      "Epoch: 023, Loss: 124594600.0\n",
      "Epoch: 024, Loss: 101180472.0\n",
      "Epoch: 025, Loss: 382756768.0\n",
      "Epoch: 026, Loss: 372196928.0\n",
      "Epoch: 027, Loss: 126243808.0\n",
      "Epoch: 028, Loss: 140264992.0\n",
      "Epoch: 029, Loss: 114259512.0\n",
      "Epoch: 030, Loss: 21320776.0\n",
      "Epoch: 031, Loss: 19001060.0\n",
      "Epoch: 032, Loss: 10679451.0\n",
      "Epoch: 033, Loss: 12800734.0\n",
      "Epoch: 034, Loss: 21214070.0\n",
      "Epoch: 035, Loss: 15628458.0\n",
      "Epoch: 036, Loss: 3245388.25\n",
      "Epoch: 037, Loss: 12966787.0\n",
      "Epoch: 038, Loss: 14255326.0\n",
      "Epoch: 039, Loss: 7910792.5\n",
      "Epoch: 040, Loss: 1792718.375\n",
      "Epoch: 041, Loss: 4611495.0\n",
      "Epoch: 042, Loss: 4914343.5\n",
      "Epoch: 043, Loss: 2194180.75\n",
      "Epoch: 044, Loss: 4800510.5\n",
      "Epoch: 045, Loss: 5364426.0\n",
      "Epoch: 046, Loss: 2162587.5\n",
      "Epoch: 047, Loss: 4470647.0\n",
      "Epoch: 048, Loss: 4330242.0\n",
      "Epoch: 049, Loss: 683398.5\n",
      "Epoch: 050, Loss: 5706741.5\n",
      "Epoch: 051, Loss: 3470353.0\n",
      "Epoch: 052, Loss: 899165.3125\n",
      "Epoch: 053, Loss: 1452269.375\n",
      "Epoch: 054, Loss: 1599079.375\n",
      "Epoch: 055, Loss: 1208688.5\n",
      "Epoch: 056, Loss: 774405.0625\n",
      "Epoch: 057, Loss: 468873.5\n",
      "Epoch: 058, Loss: 272594.6875\n",
      "Epoch: 059, Loss: 514591.90625\n",
      "Epoch: 060, Loss: 499089.96875\n",
      "Epoch: 061, Loss: 314474.15625\n",
      "Epoch: 062, Loss: 153604.671875\n",
      "Epoch: 063, Loss: 194420.546875\n",
      "Epoch: 064, Loss: 206366.390625\n",
      "Epoch: 065, Loss: 207444.8125\n",
      "Epoch: 066, Loss: 194381.3125\n",
      "Epoch: 067, Loss: 165575.75\n",
      "Epoch: 068, Loss: 125431.6875\n",
      "Epoch: 069, Loss: 84342.453125\n",
      "Epoch: 070, Loss: 55859.34765625\n",
      "Epoch: 071, Loss: 37128.30859375\n",
      "Epoch: 072, Loss: 63086.40625\n",
      "Epoch: 073, Loss: 73548.4140625\n",
      "Epoch: 074, Loss: 67179.0703125\n",
      "Epoch: 075, Loss: 49365.01171875\n",
      "Epoch: 076, Loss: 27909.69921875\n",
      "Epoch: 077, Loss: 23737.837890625\n",
      "Epoch: 078, Loss: 30910.54296875\n",
      "Epoch: 079, Loss: 31309.646484375\n",
      "Epoch: 080, Loss: 25704.837890625\n",
      "Epoch: 081, Loss: 16200.4580078125\n",
      "Epoch: 082, Loss: 19912.453125\n",
      "Epoch: 083, Loss: 23212.708984375\n",
      "Epoch: 084, Loss: 23857.275390625\n",
      "Epoch: 085, Loss: 22046.03515625\n",
      "Epoch: 086, Loss: 18339.078125\n",
      "Epoch: 087, Loss: 13923.4638671875\n",
      "Epoch: 088, Loss: 12628.7421875\n",
      "Epoch: 089, Loss: 14961.486328125\n",
      "Epoch: 090, Loss: 13209.0634765625\n",
      "Epoch: 091, Loss: 9387.1650390625\n",
      "Epoch: 092, Loss: 9685.697265625\n",
      "Epoch: 093, Loss: 9866.3046875\n",
      "Epoch: 094, Loss: 9096.8828125\n",
      "Epoch: 095, Loss: 7468.8525390625\n",
      "Epoch: 096, Loss: 6244.9853515625\n",
      "Epoch: 097, Loss: 6832.11083984375\n",
      "Epoch: 098, Loss: 6459.4208984375\n",
      "Epoch: 099, Loss: 5353.828125\n",
      "Epoch: 100, Loss: 5174.5859375\n",
      "Epoch: 101, Loss: 5354.11328125\n",
      "Epoch: 102, Loss: 4466.09765625\n",
      "Epoch: 103, Loss: 4338.8125\n",
      "Epoch: 104, Loss: 4540.6533203125\n",
      "Epoch: 105, Loss: 4085.90673828125\n",
      "Epoch: 106, Loss: 3201.346923828125\n",
      "Epoch: 107, Loss: 3713.449462890625\n",
      "Epoch: 108, Loss: 3656.425537109375\n",
      "Epoch: 109, Loss: 2702.816650390625\n",
      "Epoch: 110, Loss: 3186.101806640625\n",
      "Epoch: 111, Loss: 3228.13525390625\n",
      "Epoch: 112, Loss: 2584.608642578125\n",
      "Epoch: 113, Loss: 3153.401123046875\n",
      "Epoch: 114, Loss: 3057.966552734375\n",
      "Epoch: 115, Loss: 2514.50244140625\n",
      "Epoch: 116, Loss: 2863.666748046875\n",
      "Epoch: 117, Loss: 2470.05078125\n",
      "Epoch: 118, Loss: 2693.44873046875\n",
      "Epoch: 119, Loss: 2490.2333984375\n",
      "Epoch: 120, Loss: 2500.671875\n",
      "Epoch: 121, Loss: 2668.055908203125\n",
      "Epoch: 122, Loss: 2245.43896484375\n",
      "Epoch: 123, Loss: 2619.294189453125\n",
      "Epoch: 124, Loss: 2317.77734375\n",
      "Epoch: 125, Loss: 2449.72607421875\n",
      "Epoch: 126, Loss: 2695.689208984375\n",
      "Epoch: 127, Loss: 2347.947998046875\n",
      "Epoch: 128, Loss: 2252.00537109375\n",
      "Epoch: 129, Loss: 2270.478271484375\n",
      "Epoch: 130, Loss: 2203.720458984375\n",
      "Epoch: 131, Loss: 2350.255126953125\n",
      "Epoch: 132, Loss: 2061.390625\n",
      "Epoch: 133, Loss: 2286.652099609375\n",
      "Epoch: 134, Loss: 1990.960693359375\n",
      "Epoch: 135, Loss: 2158.463623046875\n",
      "Epoch: 136, Loss: 2184.849365234375\n",
      "Epoch: 137, Loss: 1895.3458251953125\n",
      "Epoch: 138, Loss: 2271.21728515625\n",
      "Epoch: 139, Loss: 1863.7911376953125\n",
      "Epoch: 140, Loss: 2067.43359375\n",
      "Epoch: 141, Loss: 1963.35302734375\n",
      "Epoch: 142, Loss: 1907.0533447265625\n",
      "Epoch: 143, Loss: 1857.5885009765625\n",
      "Epoch: 144, Loss: 1910.528076171875\n",
      "Epoch: 145, Loss: 1896.601318359375\n",
      "Epoch: 146, Loss: 1777.0439453125\n",
      "Epoch: 147, Loss: 1822.229248046875\n",
      "Epoch: 148, Loss: 1799.461181640625\n",
      "Epoch: 149, Loss: 1804.8511962890625\n",
      "Epoch: 150, Loss: 1719.9814453125\n",
      "Epoch: 151, Loss: 1746.7967529296875\n",
      "Epoch: 152, Loss: 1744.4705810546875\n",
      "Epoch: 153, Loss: 1727.6505126953125\n",
      "Epoch: 154, Loss: 1691.848388671875\n",
      "Epoch: 155, Loss: 1645.34130859375\n",
      "Epoch: 156, Loss: 1680.2196044921875\n",
      "Epoch: 157, Loss: 1627.1636962890625\n",
      "Epoch: 158, Loss: 1676.390869140625\n",
      "Epoch: 159, Loss: 1585.93359375\n",
      "Epoch: 160, Loss: 1605.6990966796875\n",
      "Epoch: 161, Loss: 1562.6910400390625\n",
      "Epoch: 162, Loss: 1573.089599609375\n",
      "Epoch: 163, Loss: 1559.91748046875\n",
      "Epoch: 164, Loss: 1539.357666015625\n",
      "Epoch: 165, Loss: 1552.741943359375\n",
      "Epoch: 166, Loss: 1509.70703125\n",
      "Epoch: 167, Loss: 1506.694580078125\n",
      "Epoch: 168, Loss: 1492.2724609375\n",
      "Epoch: 169, Loss: 1471.81298828125\n",
      "Epoch: 170, Loss: 1469.1558837890625\n",
      "Epoch: 171, Loss: 1449.2279052734375\n",
      "Epoch: 172, Loss: 1447.9442138671875\n",
      "Epoch: 173, Loss: 1435.2135009765625\n",
      "Epoch: 174, Loss: 1418.8408203125\n",
      "Epoch: 175, Loss: 1418.015869140625\n",
      "Epoch: 176, Loss: 1399.1480712890625\n",
      "Epoch: 177, Loss: 1385.98583984375\n",
      "Epoch: 178, Loss: 1380.5916748046875\n",
      "Epoch: 179, Loss: 1362.0836181640625\n",
      "Epoch: 180, Loss: 1351.7073974609375\n",
      "Epoch: 181, Loss: 1343.835693359375\n",
      "Epoch: 182, Loss: 1328.4344482421875\n",
      "Epoch: 183, Loss: 1319.19140625\n",
      "Epoch: 184, Loss: 1307.9610595703125\n",
      "Epoch: 185, Loss: 1294.9459228515625\n",
      "Epoch: 186, Loss: 1285.53759765625\n",
      "Epoch: 187, Loss: 1273.7017822265625\n",
      "Epoch: 188, Loss: 1264.8724365234375\n",
      "Epoch: 189, Loss: 1253.8043212890625\n",
      "Epoch: 190, Loss: 1242.853759765625\n",
      "Epoch: 191, Loss: 1234.40478515625\n",
      "Epoch: 192, Loss: 1223.52587890625\n",
      "Epoch: 193, Loss: 1211.91650390625\n",
      "Epoch: 194, Loss: 1205.9261474609375\n",
      "Epoch: 195, Loss: 1196.9595947265625\n",
      "Epoch: 196, Loss: 1182.243408203125\n",
      "Epoch: 197, Loss: 1183.3223876953125\n",
      "Epoch: 198, Loss: 1175.4261474609375\n",
      "Epoch: 199, Loss: 1154.6683349609375\n",
      "Epoch: 200, Loss: 1164.072021484375\n",
      "Epoch: 201, Loss: 1148.2095947265625\n",
      "Epoch: 202, Loss: 1129.390380859375\n",
      "Epoch: 203, Loss: 1138.3336181640625\n",
      "Epoch: 204, Loss: 1116.358642578125\n",
      "Epoch: 205, Loss: 1102.0272216796875\n",
      "Epoch: 206, Loss: 1108.106201171875\n",
      "Epoch: 207, Loss: 1087.538818359375\n",
      "Epoch: 208, Loss: 1073.6575927734375\n",
      "Epoch: 209, Loss: 1079.7811279296875\n",
      "Epoch: 210, Loss: 1060.47265625\n",
      "Epoch: 211, Loss: 1042.9681396484375\n",
      "Epoch: 212, Loss: 1054.0201416015625\n",
      "Epoch: 213, Loss: 1042.8641357421875\n",
      "Epoch: 214, Loss: 1015.4375\n",
      "Epoch: 215, Loss: 1041.6829833984375\n",
      "Epoch: 216, Loss: 1035.2086181640625\n",
      "Epoch: 217, Loss: 996.7742309570312\n",
      "Epoch: 218, Loss: 1063.52490234375\n",
      "Epoch: 219, Loss: 1035.736083984375\n",
      "Epoch: 220, Loss: 995.9741821289062\n",
      "Epoch: 221, Loss: 1080.5462646484375\n",
      "Epoch: 222, Loss: 990.5609130859375\n",
      "Epoch: 223, Loss: 985.2633666992188\n",
      "Epoch: 224, Loss: 1008.7438354492188\n",
      "Epoch: 225, Loss: 942.6490478515625\n",
      "Epoch: 226, Loss: 934.406982421875\n",
      "Epoch: 227, Loss: 959.1715087890625\n",
      "Epoch: 228, Loss: 929.0935668945312\n",
      "Epoch: 229, Loss: 899.4645385742188\n",
      "Epoch: 230, Loss: 959.2443237304688\n",
      "Epoch: 231, Loss: 936.0642700195312\n",
      "Epoch: 232, Loss: 891.0651245117188\n",
      "Epoch: 233, Loss: 988.861572265625\n",
      "Epoch: 234, Loss: 927.61572265625\n",
      "Epoch: 235, Loss: 900.4577026367188\n",
      "Epoch: 236, Loss: 970.6701049804688\n",
      "Epoch: 237, Loss: 883.3515014648438\n",
      "Epoch: 238, Loss: 871.755615234375\n",
      "Epoch: 239, Loss: 926.4120483398438\n",
      "Epoch: 240, Loss: 855.2451171875\n",
      "Epoch: 241, Loss: 832.5013427734375\n",
      "Epoch: 242, Loss: 903.8140258789062\n",
      "Epoch: 243, Loss: 861.2650146484375\n",
      "Epoch: 244, Loss: 814.3615112304688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245, Loss: 927.036376953125\n",
      "Epoch: 246, Loss: 863.291259765625\n",
      "Epoch: 247, Loss: 830.4476928710938\n",
      "Epoch: 248, Loss: 953.1025390625\n",
      "Epoch: 249, Loss: 820.6620483398438\n",
      "Epoch: 250, Loss: 804.8634033203125\n",
      "Epoch: 251, Loss: 898.9557495117188\n",
      "Epoch: 252, Loss: 812.3843383789062\n",
      "Epoch: 253, Loss: 780.9647216796875\n",
      "Epoch: 254, Loss: 902.6033935546875\n",
      "Epoch: 255, Loss: 804.8889770507812\n",
      "Epoch: 256, Loss: 781.6464233398438\n",
      "Epoch: 257, Loss: 891.2623901367188\n",
      "Epoch: 258, Loss: 767.72998046875\n",
      "Epoch: 259, Loss: 746.0284423828125\n",
      "Epoch: 260, Loss: 866.3286743164062\n",
      "Epoch: 261, Loss: 775.3165283203125\n",
      "Epoch: 262, Loss: 743.881591796875\n",
      "Epoch: 263, Loss: 868.7384033203125\n",
      "Epoch: 264, Loss: 744.3464965820312\n",
      "Epoch: 265, Loss: 718.401611328125\n",
      "Epoch: 266, Loss: 845.1798095703125\n",
      "Epoch: 267, Loss: 738.0054321289062\n",
      "Epoch: 268, Loss: 710.663818359375\n",
      "Epoch: 269, Loss: 844.6620483398438\n",
      "Epoch: 270, Loss: 709.8916625976562\n",
      "Epoch: 271, Loss: 684.0540161132812\n",
      "Epoch: 272, Loss: 826.8541259765625\n",
      "Epoch: 273, Loss: 705.9624633789062\n",
      "Epoch: 274, Loss: 675.1278076171875\n",
      "Epoch: 275, Loss: 820.3098754882812\n",
      "Epoch: 276, Loss: 673.7615356445312\n",
      "Epoch: 277, Loss: 654.5746459960938\n",
      "Epoch: 278, Loss: 788.6484375\n",
      "Epoch: 279, Loss: 673.1870727539062\n",
      "Epoch: 280, Loss: 631.2913208007812\n",
      "Epoch: 281, Loss: 803.4470825195312\n",
      "Epoch: 282, Loss: 638.3353881835938\n",
      "Epoch: 283, Loss: 615.1572875976562\n",
      "Epoch: 284, Loss: 766.87060546875\n",
      "Epoch: 285, Loss: 634.7891845703125\n",
      "Epoch: 286, Loss: 589.0892944335938\n",
      "Epoch: 287, Loss: 789.1023559570312\n",
      "Epoch: 288, Loss: 602.589111328125\n",
      "Epoch: 289, Loss: 576.7817993164062\n",
      "Epoch: 290, Loss: 748.8314819335938\n",
      "Epoch: 291, Loss: 599.2460327148438\n",
      "Epoch: 292, Loss: 555.0086059570312\n",
      "Epoch: 293, Loss: 764.8309326171875\n",
      "Epoch: 294, Loss: 567.8515625\n",
      "Epoch: 295, Loss: 537.2252197265625\n",
      "Epoch: 296, Loss: 726.3733520507812\n",
      "Epoch: 297, Loss: 560.63671875\n",
      "Epoch: 298, Loss: 510.7281494140625\n",
      "Epoch: 299, Loss: 740.7474365234375\n"
     ]
    }
   ],
   "source": [
    "train_dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bdd8615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.81      0.65       712\n",
      "         1.0       0.64      0.34      0.44       712\n",
      "\n",
      "    accuracy                           0.57      1424\n",
      "   macro avg       0.59      0.57      0.55      1424\n",
      "weighted avg       0.59      0.57      0.55      1424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dict = model4_dot(train_data.to(device))\n",
    "edge_label_index = train_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "pred = model4_dot.predict_link( x_dict['drug'], x_dict['target'], edge_label_index).squeeze()\n",
    "\n",
    "pred_prob = torch.sigmoid(pred)\n",
    "\n",
    "pred_prob_binary = pred_prob >= 0.5\n",
    "pred_prob_binary = pred_prob_binary.float()\n",
    "pred_prob_binary_np = pred_prob_binary.cpu().numpy()\n",
    "\n",
    "ground_truth = train_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "ground_truth = ground_truth.float()\n",
    "ground_truth_np = ground_truth.cpu().numpy()\n",
    "\n",
    "report = classification_report(ground_truth_np, pred_prob_binary_np)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e59b36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.63      0.55       474\n",
      "           1       0.48      0.34      0.40       474\n",
      "\n",
      "    accuracy                           0.48       948\n",
      "   macro avg       0.48      0.48      0.47       948\n",
      "weighted avg       0.48      0.48      0.47       948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dict_val = model4_dot(val_data)\n",
    "edge_label_index_val = val_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "pred_val = model4_dot.predict_link(x_dict_val['drug'], x_dict_val['target'], edge_label_index_val).squeeze()\n",
    "\n",
    "pred_val_prob = torch.sigmoid(pred_val)\n",
    "pred_prob_val_binary = pred_val_prob >= 0.5\n",
    "pred_prob_val_binary = pred_prob_val_binary.float()\n",
    "pred_prob_val_binary_np = pred_prob_val_binary.cpu().numpy()\n",
    "ground_truth_val = val_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "ground_truth_val_np = ground_truth_val.cpu().numpy()\n",
    "\n",
    "report = classification_report(ground_truth_val_np, pred_prob_val_binary_np)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87d65ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will create another model which has torch.nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e7136d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mdrug\u001b[0m={\n",
       "    node_id=[1482],\n",
       "    x=[1482, 1444]\n",
       "  },\n",
       "  \u001b[1mtarget\u001b[0m={\n",
       "    node_id=[1408],\n",
       "    x=[1408, 4159]\n",
       "  },\n",
       "  \u001b[1m(drug, interact, target)\u001b[0m={\n",
       "    edge_index=[2, 1659],\n",
       "    edge_label=[1424],\n",
       "    edge_label_index=[2, 1424]\n",
       "  },\n",
       "  \u001b[1m(drug, similar, drug)\u001b[0m={ edge_index=[2, 1246640] },\n",
       "  \u001b[1m(target, similar, target)\u001b[0m={ edge_index=[2, 1043321] },\n",
       "  \u001b[1m(target, rev_interact, drug)\u001b[0m={ edge_index=[2, 1659] }\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018f4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_5(torch.nn.Module):\n",
    "    def __init__(self, final_dimension):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(final_dimension, final_dimension)\n",
    "        self.conv2 = SAGEConv(final_dimension, final_dimension)\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x_drug: Tensor, x_target: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_drug = x_drug[edge_label_index[0]]\n",
    "        edge_feat_target = x_target[edge_label_index[1]]\n",
    "\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (edge_feat_drug * edge_feat_target).sum(dim=-1)\n",
    "    \n",
    "class Model_5_dot(torch.nn.Module):\n",
    "    def __init__(self, final_dimension):\n",
    "        super().__init__()\n",
    "        # embeddings ???\n",
    "        # self.user_lin = torch.nn.Linear(final_dimension, final_dimension)\n",
    "        self.drug_emb = torch.nn.Embedding(train_data[\"drug\"].num_nodes, final_dimension)\n",
    "        self.target_emb = torch.nn.Embedding(train_data[\"target\"].num_nodes, final_dimension)\n",
    "        \n",
    "        self.drug_lin = torch.nn.Linear(1444, final_dimension)\n",
    "        self.target_lin = torch.nn.Linear(4159, final_dimension)\n",
    "        self.shared_lin = torch.nn.Linear(final_dimension, final_dimension)\n",
    "        # Instantiate homogeneous GNN\n",
    "        self.gnn = GNN_5(final_dimension)\n",
    "        # Convert GNN model into a heterogeneous variant\n",
    "        self.gnn = to_hetero(self.gnn, metadata=train_data.metadata())\n",
    "        self.classifier = Classifier()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, data: HeteroData) -> Tensor:\n",
    "        x_dict = {\n",
    "            \"drug\": self.shared_lin(self.drug_lin(data[\"drug\"].x) + (self.drug_emb(data['drug'].node_id))),\n",
    "            \"target\": self.shared_lin(self.target_lin(data[\"target\"].x) + (self.target_emb(data['target'].node_id))),\n",
    "        \n",
    "        }\n",
    "\n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "        return x_dict\n",
    "\n",
    "    def predict_link(self, drug_emb, target_emb, edge_label_index):\n",
    "        return self.classifier(drug_emb, target_emb, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "354db776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5947e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "479b29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_dot = Model_5_dot(final_dimension=5000)\n",
    "model5_dot = model5_dot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba8f67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer_dot = Adam(model5_dot.parameters(), lr=0.001)\n",
    "loss_fn_dot = torch.nn.BCEWithLogitsLoss() # ????BCEwithLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4c739eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dot():\n",
    "    model5_dot.train()\n",
    "    for epoch in range(1, 300):\n",
    "        optimizer_dot.zero_grad()\n",
    "        x_dict = model5_dot(train_data)\n",
    "        edge_label_index = train_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "        ground_truth = train_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "        ground_truth = ground_truth.float()\n",
    "        pred = model5_dot.predict_link( x_dict['drug'], x_dict['target'], edge_label_index).squeeze()\n",
    "        # print(type(pred[0].item()))\n",
    "        # print(type(ground_truth[0].item()))\n",
    "        loss = loss_fn_dot(pred, ground_truth)\n",
    "        loss.backward()\n",
    "        optimizer_dot.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Epoch: {epoch:03d}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb4df115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 60.737754821777344\n",
      "Epoch: 002, Loss: 15420651.0\n",
      "Epoch: 003, Loss: 7756938240.0\n",
      "Epoch: 004, Loss: 482417504.0\n",
      "Epoch: 005, Loss: 71965840.0\n",
      "Epoch: 006, Loss: 545572544.0\n",
      "Epoch: 007, Loss: 194074368.0\n",
      "Epoch: 008, Loss: 2349987584.0\n",
      "Epoch: 009, Loss: 274188832.0\n",
      "Epoch: 010, Loss: 2452683008.0\n",
      "Epoch: 011, Loss: 209993859072.0\n",
      "Epoch: 012, Loss: 1093994112.0\n",
      "Epoch: 013, Loss: 15195159552.0\n",
      "Epoch: 014, Loss: 2947631872.0\n",
      "Epoch: 015, Loss: 1063228288.0\n",
      "Epoch: 016, Loss: 26001235968.0\n",
      "Epoch: 017, Loss: 31820266.0\n",
      "Epoch: 018, Loss: 2393083136.0\n",
      "Epoch: 019, Loss: 9467177984.0\n",
      "Epoch: 020, Loss: 5389392896.0\n",
      "Epoch: 021, Loss: 669510016.0\n",
      "Epoch: 022, Loss: 625241.875\n",
      "Epoch: 023, Loss: 1584435.25\n",
      "Epoch: 024, Loss: 13199970.0\n",
      "Epoch: 025, Loss: 101206112.0\n",
      "Epoch: 026, Loss: 480967040.0\n",
      "Epoch: 027, Loss: 395929120.0\n",
      "Epoch: 028, Loss: 585915840.0\n",
      "Epoch: 029, Loss: 79868528.0\n",
      "Epoch: 030, Loss: 43474020.0\n",
      "Epoch: 031, Loss: 318076000.0\n",
      "Epoch: 032, Loss: 9544590.0\n",
      "Epoch: 033, Loss: 28067064.0\n",
      "Epoch: 034, Loss: 91408304.0\n",
      "Epoch: 035, Loss: 49855372.0\n",
      "Epoch: 036, Loss: 52033544.0\n",
      "Epoch: 037, Loss: 51980624.0\n",
      "Epoch: 038, Loss: 42148056.0\n",
      "Epoch: 039, Loss: 24588984.0\n",
      "Epoch: 040, Loss: 33955080.0\n",
      "Epoch: 041, Loss: 32335412.0\n",
      "Epoch: 042, Loss: 14221315.0\n",
      "Epoch: 043, Loss: 16959580.0\n",
      "Epoch: 044, Loss: 19287704.0\n",
      "Epoch: 045, Loss: 17781818.0\n",
      "Epoch: 046, Loss: 12202558.0\n",
      "Epoch: 047, Loss: 11388199.0\n",
      "Epoch: 048, Loss: 9978450.0\n",
      "Epoch: 049, Loss: 11620887.0\n",
      "Epoch: 050, Loss: 15514987.0\n",
      "Epoch: 051, Loss: 14424809.0\n",
      "Epoch: 052, Loss: 9351204.0\n",
      "Epoch: 053, Loss: 2827566.0\n",
      "Epoch: 054, Loss: 7133028.5\n",
      "Epoch: 055, Loss: 5160824.5\n",
      "Epoch: 056, Loss: 3155284.5\n",
      "Epoch: 057, Loss: 7482625.5\n",
      "Epoch: 058, Loss: 5771827.0\n",
      "Epoch: 059, Loss: 1182716.0\n",
      "Epoch: 060, Loss: 2617646.5\n",
      "Epoch: 061, Loss: 1499657.875\n",
      "Epoch: 062, Loss: 1324563.375\n",
      "Epoch: 063, Loss: 1990848.0\n",
      "Epoch: 064, Loss: 979694.5625\n",
      "Epoch: 065, Loss: 1311956.25\n",
      "Epoch: 066, Loss: 1603051.375\n",
      "Epoch: 067, Loss: 1138520.25\n",
      "Epoch: 068, Loss: 804786.5\n",
      "Epoch: 069, Loss: 1187873.875\n",
      "Epoch: 070, Loss: 557479.625\n",
      "Epoch: 071, Loss: 1170780.375\n",
      "Epoch: 072, Loss: 1330035.75\n",
      "Epoch: 073, Loss: 747074.625\n",
      "Epoch: 074, Loss: 1111065.75\n",
      "Epoch: 075, Loss: 1296858.125\n",
      "Epoch: 076, Loss: 423895.8125\n",
      "Epoch: 077, Loss: 1133785.75\n",
      "Epoch: 078, Loss: 1225645.5\n",
      "Epoch: 079, Loss: 669202.4375\n",
      "Epoch: 080, Loss: 742932.25\n",
      "Epoch: 081, Loss: 959300.75\n",
      "Epoch: 082, Loss: 384791.1875\n",
      "Epoch: 083, Loss: 757815.4375\n",
      "Epoch: 084, Loss: 860179.5\n",
      "Epoch: 085, Loss: 603246.3125\n",
      "Epoch: 086, Loss: 356383.21875\n",
      "Epoch: 087, Loss: 544050.1875\n",
      "Epoch: 088, Loss: 236859.84375\n",
      "Epoch: 089, Loss: 394321.0\n",
      "Epoch: 090, Loss: 428812.0625\n",
      "Epoch: 091, Loss: 213861.53125\n",
      "Epoch: 092, Loss: 452176.09375\n",
      "Epoch: 093, Loss: 414785.96875\n",
      "Epoch: 094, Loss: 255052.828125\n",
      "Epoch: 095, Loss: 296841.21875\n",
      "Epoch: 096, Loss: 124249.71875\n",
      "Epoch: 097, Loss: 256233.703125\n",
      "Epoch: 098, Loss: 159922.109375\n",
      "Epoch: 099, Loss: 237695.421875\n",
      "Epoch: 100, Loss: 186435.796875\n",
      "Epoch: 101, Loss: 226475.03125\n",
      "Epoch: 102, Loss: 148451.234375\n",
      "Epoch: 103, Loss: 171622.15625\n",
      "Epoch: 104, Loss: 166118.25\n",
      "Epoch: 105, Loss: 146097.3125\n",
      "Epoch: 106, Loss: 147731.90625\n",
      "Epoch: 107, Loss: 150500.796875\n",
      "Epoch: 108, Loss: 104940.5859375\n",
      "Epoch: 109, Loss: 140946.390625\n",
      "Epoch: 110, Loss: 119139.15625\n",
      "Epoch: 111, Loss: 140285.796875\n",
      "Epoch: 112, Loss: 113113.640625\n",
      "Epoch: 113, Loss: 111774.3359375\n",
      "Epoch: 114, Loss: 100108.1171875\n",
      "Epoch: 115, Loss: 124405.125\n",
      "Epoch: 116, Loss: 99587.7109375\n",
      "Epoch: 117, Loss: 90109.4609375\n",
      "Epoch: 118, Loss: 104798.65625\n",
      "Epoch: 119, Loss: 99578.1328125\n",
      "Epoch: 120, Loss: 105122.515625\n",
      "Epoch: 121, Loss: 81794.7109375\n",
      "Epoch: 122, Loss: 99448.40625\n",
      "Epoch: 123, Loss: 86249.1328125\n",
      "Epoch: 124, Loss: 83286.640625\n",
      "Epoch: 125, Loss: 90757.3515625\n",
      "Epoch: 126, Loss: 78919.796875\n",
      "Epoch: 127, Loss: 91917.6953125\n",
      "Epoch: 128, Loss: 80230.7109375\n",
      "Epoch: 129, Loss: 82968.71875\n",
      "Epoch: 130, Loss: 87710.359375\n",
      "Epoch: 131, Loss: 74706.8125\n",
      "Epoch: 132, Loss: 79784.2734375\n",
      "Epoch: 133, Loss: 79616.5234375\n",
      "Epoch: 134, Loss: 76265.6953125\n",
      "Epoch: 135, Loss: 79218.796875\n",
      "Epoch: 136, Loss: 70340.4921875\n",
      "Epoch: 137, Loss: 77042.3203125\n",
      "Epoch: 138, Loss: 74918.1796875\n",
      "Epoch: 139, Loss: 69087.84375\n",
      "Epoch: 140, Loss: 71659.3125\n",
      "Epoch: 141, Loss: 72262.1484375\n",
      "Epoch: 142, Loss: 67594.1484375\n",
      "Epoch: 143, Loss: 67831.1875\n",
      "Epoch: 144, Loss: 70065.03125\n",
      "Epoch: 145, Loss: 67588.7265625\n",
      "Epoch: 146, Loss: 67031.8203125\n",
      "Epoch: 147, Loss: 67341.0234375\n",
      "Epoch: 148, Loss: 66431.1328125\n",
      "Epoch: 149, Loss: 66065.2265625\n",
      "Epoch: 150, Loss: 64671.5234375\n",
      "Epoch: 151, Loss: 63363.6796875\n",
      "Epoch: 152, Loss: 64106.484375\n",
      "Epoch: 153, Loss: 63423.796875\n",
      "Epoch: 154, Loss: 61910.59765625\n",
      "Epoch: 155, Loss: 62301.10546875\n",
      "Epoch: 156, Loss: 62315.44921875\n",
      "Epoch: 157, Loss: 60689.74609375\n",
      "Epoch: 158, Loss: 60961.88671875\n",
      "Epoch: 159, Loss: 61144.06640625\n",
      "Epoch: 160, Loss: 60251.796875\n",
      "Epoch: 161, Loss: 59664.60546875\n",
      "Epoch: 162, Loss: 59026.6953125\n",
      "Epoch: 163, Loss: 58310.3828125\n",
      "Epoch: 164, Loss: 58483.2265625\n",
      "Epoch: 165, Loss: 58217.19140625\n",
      "Epoch: 166, Loss: 57433.01171875\n",
      "Epoch: 167, Loss: 57291.13671875\n",
      "Epoch: 168, Loss: 56887.15625\n",
      "Epoch: 169, Loss: 56700.7421875\n",
      "Epoch: 170, Loss: 56907.51171875\n",
      "Epoch: 171, Loss: 56113.60546875\n",
      "Epoch: 172, Loss: 55678.6171875\n",
      "Epoch: 173, Loss: 55305.2265625\n",
      "Epoch: 174, Loss: 54508.96875\n",
      "Epoch: 175, Loss: 53783.78515625\n",
      "Epoch: 176, Loss: 53626.42578125\n",
      "Epoch: 177, Loss: 53261.11328125\n",
      "Epoch: 178, Loss: 53016.9453125\n",
      "Epoch: 179, Loss: 52988.88671875\n",
      "Epoch: 180, Loss: 52998.6171875\n",
      "Epoch: 181, Loss: 54265.5078125\n",
      "Epoch: 182, Loss: 55067.27734375\n",
      "Epoch: 183, Loss: 54186.60546875\n",
      "Epoch: 184, Loss: 52877.93359375\n",
      "Epoch: 185, Loss: 52463.4375\n",
      "Epoch: 186, Loss: 52486.05859375\n",
      "Epoch: 187, Loss: 53195.42578125\n",
      "Epoch: 188, Loss: 54149.0078125\n",
      "Epoch: 189, Loss: 54274.203125\n",
      "Epoch: 190, Loss: 53947.53515625\n",
      "Epoch: 191, Loss: 53048.77734375\n",
      "Epoch: 192, Loss: 52341.0234375\n",
      "Epoch: 193, Loss: 51835.42578125\n",
      "Epoch: 194, Loss: 51793.359375\n",
      "Epoch: 195, Loss: 53576.83203125\n",
      "Epoch: 196, Loss: 56956.80859375\n",
      "Epoch: 197, Loss: 60794.796875\n",
      "Epoch: 198, Loss: 65182.83203125\n",
      "Epoch: 199, Loss: 58642.0390625\n",
      "Epoch: 200, Loss: 50527.46875\n",
      "Epoch: 201, Loss: 45770.8671875\n",
      "Epoch: 202, Loss: 45657.125\n",
      "Epoch: 203, Loss: 47570.09765625\n",
      "Epoch: 204, Loss: 56445.21484375\n",
      "Epoch: 205, Loss: 75615.9140625\n",
      "Epoch: 206, Loss: 55748.21484375\n",
      "Epoch: 207, Loss: 47601.5625\n",
      "Epoch: 208, Loss: 58852.5390625\n",
      "Epoch: 209, Loss: 81153.4375\n",
      "Epoch: 210, Loss: 89327.78125\n",
      "Epoch: 211, Loss: 52838.92578125\n",
      "Epoch: 212, Loss: 49149.6953125\n",
      "Epoch: 213, Loss: 54818.9765625\n",
      "Epoch: 214, Loss: 57114.1015625\n",
      "Epoch: 215, Loss: 81902.015625\n",
      "Epoch: 216, Loss: 83485.8359375\n",
      "Epoch: 217, Loss: 66855.25\n",
      "Epoch: 218, Loss: 48870.3203125\n",
      "Epoch: 219, Loss: 86715.46875\n",
      "Epoch: 220, Loss: 89414.5546875\n",
      "Epoch: 221, Loss: 43791.92578125\n",
      "Epoch: 222, Loss: 94819.5625\n",
      "Epoch: 223, Loss: 81348.5703125\n",
      "Epoch: 224, Loss: 47100.9609375\n",
      "Epoch: 225, Loss: 116516.2734375\n",
      "Epoch: 226, Loss: 51108.5078125\n",
      "Epoch: 227, Loss: 52623.85546875\n",
      "Epoch: 228, Loss: 109691.2109375\n",
      "Epoch: 229, Loss: 45003.1484375\n",
      "Epoch: 230, Loss: 61361.60546875\n",
      "Epoch: 231, Loss: 82036.921875\n",
      "Epoch: 232, Loss: 46658.16015625\n",
      "Epoch: 233, Loss: 44688.48828125\n",
      "Epoch: 234, Loss: 73845.171875\n",
      "Epoch: 235, Loss: 63326.12890625\n",
      "Epoch: 236, Loss: 41973.57421875\n",
      "Epoch: 237, Loss: 51694.8984375\n",
      "Epoch: 238, Loss: 53987.28125\n",
      "Epoch: 239, Loss: 37956.88671875\n",
      "Epoch: 240, Loss: 48176.65625\n",
      "Epoch: 241, Loss: 62897.63671875\n",
      "Epoch: 242, Loss: 54844.9453125\n",
      "Epoch: 243, Loss: 52437.34765625\n",
      "Epoch: 244, Loss: 51752.15625\n",
      "Epoch: 245, Loss: 42192.5546875\n",
      "Epoch: 246, Loss: 60215.87890625\n",
      "Epoch: 247, Loss: 51380.46875\n",
      "Epoch: 248, Loss: 45986.10546875\n",
      "Epoch: 249, Loss: 43162.66796875\n",
      "Epoch: 250, Loss: 52121.91015625\n",
      "Epoch: 251, Loss: 50805.62890625\n",
      "Epoch: 252, Loss: 35343.55078125\n",
      "Epoch: 253, Loss: 44890.875\n",
      "Epoch: 254, Loss: 59190.515625\n",
      "Epoch: 255, Loss: 45506.265625\n",
      "Epoch: 256, Loss: 46881.96875\n",
      "Epoch: 257, Loss: 49465.4375\n",
      "Epoch: 258, Loss: 34361.40625\n",
      "Epoch: 259, Loss: 47363.05859375\n",
      "Epoch: 260, Loss: 36925.71875\n",
      "Epoch: 261, Loss: 43003.015625\n",
      "Epoch: 262, Loss: 49506.28515625\n",
      "Epoch: 263, Loss: 40323.3046875\n",
      "Epoch: 264, Loss: 32685.609375\n",
      "Epoch: 265, Loss: 48565.98828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 266, Loss: 60932.203125\n",
      "Epoch: 267, Loss: 60106.6953125\n",
      "Epoch: 268, Loss: 47818.95703125\n",
      "Epoch: 269, Loss: 89859.6328125\n",
      "Epoch: 270, Loss: 36502.8203125\n",
      "Epoch: 271, Loss: 54431.515625\n",
      "Epoch: 272, Loss: 83736.2890625\n",
      "Epoch: 273, Loss: 30925.689453125\n",
      "Epoch: 274, Loss: 55234.62890625\n",
      "Epoch: 275, Loss: 49019.44921875\n",
      "Epoch: 276, Loss: 33332.8828125\n",
      "Epoch: 277, Loss: 33784.5859375\n",
      "Epoch: 278, Loss: 53843.3046875\n",
      "Epoch: 279, Loss: 58206.05859375\n",
      "Epoch: 280, Loss: 36239.71484375\n",
      "Epoch: 281, Loss: 40771.51171875\n",
      "Epoch: 282, Loss: 42320.17578125\n",
      "Epoch: 283, Loss: 29839.607421875\n",
      "Epoch: 284, Loss: 40549.640625\n",
      "Epoch: 285, Loss: 28941.146484375\n",
      "Epoch: 286, Loss: 33118.421875\n",
      "Epoch: 287, Loss: 35462.828125\n",
      "Epoch: 288, Loss: 28943.80859375\n",
      "Epoch: 289, Loss: 33578.08984375\n",
      "Epoch: 290, Loss: 44349.48828125\n",
      "Epoch: 291, Loss: 52001.4375\n",
      "Epoch: 292, Loss: 34344.34375\n",
      "Epoch: 293, Loss: 62717.05859375\n",
      "Epoch: 294, Loss: 50779.8203125\n",
      "Epoch: 295, Loss: 42287.56640625\n",
      "Epoch: 296, Loss: 94725.9765625\n",
      "Epoch: 297, Loss: 30337.47265625\n",
      "Epoch: 298, Loss: 108520.1796875\n",
      "Epoch: 299, Loss: 54742.85546875\n"
     ]
    }
   ],
   "source": [
    "train_dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ecb7d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.09      0.15       712\n",
      "         1.0       0.50      0.93      0.65       712\n",
      "\n",
      "    accuracy                           0.51      1424\n",
      "   macro avg       0.53      0.51      0.40      1424\n",
      "weighted avg       0.53      0.51      0.40      1424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dict = model5_dot(train_data.to(device))\n",
    "edge_label_index = train_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "pred = model5_dot.predict_link( x_dict['drug'], x_dict['target'], edge_label_index).squeeze()\n",
    "\n",
    "pred_prob = torch.sigmoid(pred)\n",
    "\n",
    "pred_prob_binary = pred_prob >= 0.5\n",
    "pred_prob_binary = pred_prob_binary.float()\n",
    "pred_prob_binary_np = pred_prob_binary.cpu().numpy()\n",
    "\n",
    "ground_truth = train_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "ground_truth = ground_truth.float()\n",
    "ground_truth_np = ground_truth.cpu().numpy()\n",
    "\n",
    "report = classification_report(ground_truth_np, pred_prob_binary_np)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "521077d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.49      0.49       474\n",
      "           1       0.50      0.51      0.50       474\n",
      "\n",
      "    accuracy                           0.50       948\n",
      "   macro avg       0.50      0.50      0.50       948\n",
      "weighted avg       0.50      0.50      0.50       948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_dict_val = model5_dot(val_data)\n",
    "edge_label_index_val = val_data[\"drug\", \"interact\", \"target\"].edge_label_index\n",
    "pred_val = model5_dot.predict_link(x_dict_val['drug'], x_dict_val['target'], edge_label_index_val).squeeze()\n",
    "\n",
    "pred_val_prob = torch.sigmoid(pred_val)\n",
    "pred_prob_val_binary = pred_val_prob >= 0.5\n",
    "pred_prob_val_binary = pred_prob_val_binary.float()\n",
    "pred_prob_val_binary_np = pred_prob_val_binary.cpu().numpy()\n",
    "ground_truth_val = val_data[\"drug\", \"interact\", \"target\"].edge_label\n",
    "ground_truth_val_np = ground_truth_val.cpu().numpy()\n",
    "\n",
    "report = classification_report(ground_truth_val_np, pred_prob_val_binary_np)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3dcd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
